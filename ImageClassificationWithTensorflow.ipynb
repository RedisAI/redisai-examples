{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9ffdce",
   "metadata": {},
   "source": [
    "# Image Classification with Tensorflow (1.x)\n",
    "\n",
    "This example is built with Tensorflow 1.5 (tensorflow 2.x support for RedisAI is can be implemented using graph freezing. Checkout the [documentation](https://github.com/RedisAI/RedisAI)). We first take a prebuilt model from Tensorflow hub and then push that to RedisAI for production. Towards the end of this example, we also show how the loaded model can be used for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26841334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import json\n",
    "import time\n",
    "from redisai import Client\n",
    "from ml2rt import load_model, load_script, save_tensorflow\n",
    "from skimage import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f501f18",
   "metadata": {},
   "source": [
    "## Downloading and save the model\n",
    "The pretrained model is downloaded from the tensorlfow hub. We then use a dummy input `image` to serialize the graph into the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc2e4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 272 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 272 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted 272 variables to const ops.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted 272 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://tfhub.dev/google/imagenet/resnet_v2_50/classification/1'\n",
    "images = tf.placeholder(tf.float32, shape=(1, 224, 224, 3), name='images')\n",
    "module = hub.Module(url)\n",
    "logits = module(images)\n",
    "logits = tf.identity(logits, 'output')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([tf.global_variables_initializer()])\n",
    "    save_tensorflow(sess, 'resnet50.pb', output=['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d45a93",
   "metadata": {},
   "source": [
    "## Setup RedisAI\n",
    "This tutorial assumes you already have a RedisAI server running. The easiest way to setup one instance is using docker\n",
    "\n",
    "```\n",
    "docker run -p 6379:6379 redislabs/redisai:latest-cpu-x64-bionic\n",
    "```\n",
    "\n",
    "Take a look at this [quickstart](https://oss.redis.com/redisai/quickstart/) for more details. Here we setup the connection credentials and ping the server to verify we can talk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be4e169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\")\n",
    "REDIS_PORT = int(os.getenv(\"REDIS_PORT\", 6379))\n",
    "con = Client(host=REDIS_HOST, port=REDIS_PORT)\n",
    "con.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea511bfc",
   "metadata": {},
   "source": [
    "## Load model\n",
    "Next step is to load the model we trained above into RedisAI for serving. We are using a convinent package [ml2rt](https://pypi.org/project/ml2rt/) here for loading but it's not a mandatory dependency if you want to keep your `requirements.txt` small. Take a look at the `load_model` function. This will give us a binary blob of the model we have built above. We need to send this to RedisAI and also inform which backend we'd like to use and which device this should run on. We'll set the model on a key so we can reference this key later\n",
    "\n",
    "Note: If you want to run on GPU, take a look at the above quick start to setup RedisAI on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf775104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model(\"resnet50.pb\")\n",
    "con.modelstore(\"tensorflow_model\", backend=\"TF\", device=\"CPU\", inputs=['images'], outputs=['output'], data=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5fdd8c",
   "metadata": {},
   "source": [
    "## Load script\n",
    "Why do you need Script? It's very likely that your deep learning model would have a pre/post processing step, like changing the dimensionality of the input (adding batch dimension) or doing normalizatoin etc. You normally do this from your client code and send the processed data to model server. With script, you can club this into your model serving pipeline. Script is one of the powerful feature of RedisAI. RedisAI Scripts are built on top of [TorchScript](https://pytorch.org/docs/stable/jit.html) and it's recommended to take a look if TorcScript is new to you. Torchscript is a subset of python programming langauge i.e it looks and smells like python but all the python functionalities are not available in torchscript. Now if you are wondering what's the benefit of TorchScript in RedisAI, there are few\n",
    "\n",
    "- It runs on a highly effecient C++ runtime\n",
    "- It can pipeline your preprocessing and postprocessing jobs, right where your model and data resides. So no back and forth of huge data blobs between your model server and pre/post processing scripts\n",
    "- It can run in a single redis pipeline or in RedisAI Dag which makes serving channel implementation smooth\n",
    "- As in this example, even if your model is in Tensorflow, you can use Script to pipe the worflow\n",
    "\n",
    "You can load the script from a file (`ml2rt.load_script` does this for you) which is probably your workflow normally since you save the script in a file but here we pass the string into the `scriptstore` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f097999c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script = \"\"\"\n",
    "def pre_process(tensors: List[Tensor], keys: List[str], args: List[str]):\n",
    "    image = tensors[0]\n",
    "    return image.float().div(255).unsqueeze(0)\n",
    "\n",
    "def post_process(tensors: List[Tensor], keys: List[str], args: List[str]):\n",
    "    output = tensors[0]\n",
    "    # tf model has 1001 classes, hence negative 1\n",
    "    return output.max(1)[1] - 1\n",
    "\"\"\"\n",
    "con.scriptstore(\"processing_script\", device=\"CPU\", script=script, entry_points=(\"pre_process\", \"post_process\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9f2834",
   "metadata": {},
   "source": [
    "## Load the image and final classes\n",
    "Here we load the input image and the final classes to find the predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bef00e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = io.imread(\"data/cat.jpg\")\n",
    "class_idx = json.load(open(\"data/imagenet_classes.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44013e64",
   "metadata": {},
   "source": [
    "## Run the model serving pipeline\n",
    "Here we run the serving pipeline one by one and finally fetch the results out. The pipeline is organized into 5 steps\n",
    "\n",
    "```\n",
    "Setting Input -> Pre-processing Script -> Running Model -> Post-processing Script -> Fetching Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a3a24ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281 tabby, tabby catamount\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hhsecond/asgard/redisai-examples/venv/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated method scriptrun. (Use scriptexecute instead) -- Deprecated since version 1.2.0.\n",
      "  \n",
      "/Users/hhsecond/asgard/redisai-examples/venv/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated method modelrun. (Use modelexecute instead) -- Deprecated since version 1.2.0.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/hhsecond/asgard/redisai-examples/venv/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated method scriptrun. (Use scriptexecute instead) -- Deprecated since version 1.2.0.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "con.tensorset('image', image)\n",
    "out4 = con.scriptrun('processing_script', 'pre_process', inputs='image', outputs='processed')\n",
    "out5 = con.modelrun('tensorflow_model', 'processed', 'model_out')\n",
    "out6 = con.scriptrun('processing_script', 'post_process', 'model_out', 'final')\n",
    "final = con.tensorget('final')\n",
    "print(final[0], class_idx[str(final[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa981c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
